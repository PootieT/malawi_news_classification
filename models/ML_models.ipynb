{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf18363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "file = open(\"train.pkl\",'rb')\n",
    "train_representations = pickle.load(file)\n",
    "file = open(\"test.pkl\",'rb')\n",
    "test_file = pickle.load(file)\n",
    "\n",
    "train_df=pd.read_csv('C:/Users/leose/homework/malawi_news_classification/data/train.csv')\n",
    "test_df=pd.read_csv('C:/Users/leose/homework/malawi_news_classification/data/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b56a8ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leose\\anaconda3\\envs\\data\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The label must consist of integer labels of form 0, 1, 2, ..., [num_class - 1].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m lgr_model\u001b[38;5;241m=\u001b[39mlgr_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     27\u001b[0m rand_forest\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m---> 28\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test) \n\u001b[0;32m     31\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [value \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m y_pred]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data\\lib\\site-packages\\xgboost\\core.py:506\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    505\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data\\lib\\site-packages\\xgboost\\sklearn.py:1199\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_label_encoder \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_))\n\u001b[0;32m   1198\u001b[0m     ):\n\u001b[1;32m-> 1199\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(label_encoding_check_error)\n\u001b[0;32m   1201\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: The label must consist of integer labels of form 0, 1, 2, ..., [num_class - 1]."
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "X=np.array(train_representations['input_ids'][0:1400])\n",
    "\n",
    "\n",
    "y=train_df['Label'][0:1400]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(X,y)\n",
    "\n",
    "xgb_model = XGBClassifier() \n",
    "lgr_model= LogisticRegression(random_state=0)\n",
    "rand_forest = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lgr_model=lgr_model.fit(X_train, y_train)\n",
    "    rand_forest.fit(X_train, y_train)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = xgb_model.predict(X_test) \n",
    "    predictions = [value for value in y_pred]\n",
    "    accuracy = accuracy_score(y_test, predictions) \n",
    "    print(accuracy)\n",
    "    \n",
    "    \n",
    "    y_pred = lgr_model.predict(X_test) \n",
    "    predictions = [value for value in y_pred]\n",
    "    accuracy = accuracy_score(y_test, predictions) \n",
    "    print(accuracy)\n",
    "    \n",
    "    \n",
    "    y_pred = rand_forest.predict(X_test) \n",
    "    predictions = [value for value in y_pred]\n",
    "    accuracy = accuracy_score(y_test, predictions) \n",
    "    print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3c463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
